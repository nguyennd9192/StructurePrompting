import pandas as pd 
import numpy as np
import math
try:
    from scipy.misc import comb
except:
    from scipy.special import comb
import itertools
from collections import Counter
from sklearn.metrics import accuracy_score
from sklearn.base import BaseEstimator


class MassFunction(dict):
    """
    A Dempster-Shafer mass function (basic probability assignment) based on a dictionary.
    Hypotheses and their associated mass values can be added/changed/removed using the standard dictionary methods.
    Each hypothesis can be an arbitrary sequence which is automatically converted to a 'frozenset', meaning its elements must be hashable.
    """

    def __init__(self, source=None, coreset=None, cold_start="unknow", r=40):
        """
        Creates a new mass function.

        It be a dictionary mapping hypotheses to non-negative mass values
        If 'source' is not None, it is used to initialize the mass function depend on the option of cold start.
        If choosing 'unknow', the remain of mass value be assigned for the core set. 
        Otherwise, the mass values of non-focal set were assigned the mean of the remaining of mass value.
        """
        total_mass = 0
        self.coreset = coreset
        self.r = r
        if source != None:
            for (h, v) in source:
                self[frozenset(h)] = np.float64(v)
                total_mass += np.float64(v)

            if self.coreset == None:
                self.coreset = self.core()

        if self.coreset != None:
            if cold_start == "equal":
                initial_mass_value = np.float64((1.0 - total_mass) / (np.power(2, len(self.coreset)) - 1 - len(self)))
                for i in range(len(self.coreset)):
                    for hypothesis_set in itertools.combinations(self.coreset,i+1):
                        if frozenset(hypothesis_set) not in self:
                            self[frozenset(hypothesis_set)] = initial_mass_value
            elif cold_start == "unknow":
                for i in range(len(self.coreset)):
                    for hypothesis_set in itertools.combinations(self.coreset,i+1):
                        if frozenset(hypothesis_set) not in self:
                            self[frozenset(hypothesis_set)] = np.float64(0)
                self[frozenset(self.coreset)] = np.float64(1.0 - total_mass + self[frozenset(self.coreset)])
        # self.__round__()

    def focal(self): 
        """
        Returns the set of all focal hypotheses.

        A focal hypothesis has a mass value greater than 0.
        """
        return {h for (h, v) in self.items() if v > 0}

    def core(self):
        """
        Returns the core of a mass functions as a 'frozenset'.

        The core of a mass function is the union of all its focal hypotheses.
        In case a mass function does not contain any focal hypotheses, its core is an empty set.
        """
        focal = self.focal()
        if not focal:
            return frozenset()
        else:
            return frozenset.union(*focal)

    def combine(self, mass_function):
        """
        Returns a mass function was generated by combining these two mass funciton (self and mass function)

        The function use combination rules in theory of evidence for itegrating evidence.
        """
        if self.coreset != mass_function.coreset:
            raise TypeError("expected core set of the MassFunctions are the same but got two difference core set: {} and {}".format(self.coreset, mass_function.coreset))
        combined = self
        if isinstance(mass_function, MassFunction):
            mass_function = [mass_function] # wrap single mass function
        for m in mass_function:
            if not isinstance(m, MassFunction):
                raise TypeError("expected type MassFunction but got %s; make sure to use keyword arguments for anything other than mass functions" % type(m))
            combined = combined.__combine_dempster_rule__(m)
        # combined.__round__()
        return combined

    def __combine_dempster_rule__(self, mass_function):
        """
        Returns a mass function was combined using Dempster's Rule

        The function use Dempster's Rule  for combining evidence.
        """
        # print("Test")
        combined = MassFunction(coreset=self.coreset)
        # print(combined)
        for (h, v) in self.items():
            combined[h] = 0
        total_mass = 0.0
        for (h1, v1) in self.items():
            for (h2, v2) in mass_function.items():
                if len(frozenset.intersection(h1, h2)) != 0:
                    combined[frozenset.intersection(h1, h2)] += (v1 * v2)
                    total_mass += (v1 * v2)

        # print(combined, total_mass)
        for (h, v) in combined.items():
            combined[h] = combined[h] / total_mass
        return combined

    def __solve_totally_conflict__(self):
        """
        Solving the total conflict between mass functions by adding an epsilon to the Omega set.
        """
        if self[frozenset(self.coreset)] == 0.0:
            max_hypothesis = None
            max_value = 0.0
            for (h, v) in self.items():
                if v > max_value:
                    max_value = v
                    max_hypothesis = h

            self[frozenset(self.coreset)] = 1e-32
            self[max_hypothesis] = self[max_hypothesis] - 1e-32

    def __round__(self):
        """
        Returns a rouned mass function

        The function use round function for aproximating the mass values in the mass function.
        The parameter 'r' defined the number of decimal for rounding the final mass function.
        """

        total_mass = 0
        # print("----------Round-----------")
        # print(self)
        for i in range(len(self.coreset) - 1):
            for hypothesis_set in itertools.combinations(self.coreset,i + 1):
                self[frozenset(hypothesis_set)] = round(self[frozenset(hypothesis_set)], self.r)
                total_mass = total_mass + self[frozenset(hypothesis_set)]
        # print(total_mass)
        self[frozenset(self.coreset)] = 1.0 - total_mass
        self.__solve_totally_conflict__()
        # print(self)
        # print("---------------------")
        
    def pignistic(self):
        betp = []
        for coreset in self.coreset:
            tmp_betp = 0
            for (h, v) in self.items():
                if coreset in h:
                    tmp_betp += v/len(h)
            betp.append(tmp_betp) 
        return betp
                        
class SimilarityCombinationElement(object):
    """
    A module, which is based on Dempster-Shafer theory, measures similarity between (1) elements, (2) element and combination of element, and (3) combinations of element.
    The module uses mass functions to model pieces of evidence about the similarity, which are collected from the data set.
    It should be noted that the instance in work is a multi-principal-element alloy (list of element).
    This module provides two running modes: single or parallel (using Spark).
    """

    def __init__(self, data, rage_size_subset=1, seperate_symbol="|", alpha=0.1, core_set=None):
        """
        Creates a new object to measure the similarity.

        data: set of counters (name of item and number of the item) representing data instances.
        rage_size_subset: max length of the combinations of element which are considered about their similarity.
        seperate_symbol: symbol is used to separate elements in the instances of the data set.
        alpha (or alpha): a hyperparameter is used to handle the conflict between pieces of evidence.
        """
        self.data = data
        self.seperate_symbol = seperate_symbol
        self.rage_size_subset = rage_size_subset
        self.alpha = alpha
        self.core_set = core_set
        self.subsets = self.__generate_subsets__()
        self.__generate_similarity_matrices__()

    def __generate_subsets__(self):
        """
        Return list of subsets (elements or combinations of elements) is considered about their similarity with each other.
        """
        subsets = []
        for size_subset in range(self.rage_size_subset + 1):
            for subset in itertools.combinations_with_replacement(self.core_set, size_subset):
                subsets.append(subset)
        return subsets
    
    def __generate_similarity_matrices__(self):
        """
        Return data frames containing data frames contain similarity, dissimilarity and uncertainty between considered subsets.
        """
        n_subsets = len(self.subsets)
        columns_name = [self.seperate_symbol.join(sorted(k)) for k in self.subsets]

        similarity_matrix = np.zeros((n_subsets,n_subsets))
        np.fill_diagonal(similarity_matrix, 1)
        df_similarity = pd.DataFrame(similarity_matrix, columns=columns_name, index=columns_name)

        dissimilarity_matrix = np.zeros((n_subsets,n_subsets))
        df_dissimilarity = pd.DataFrame(dissimilarity_matrix, columns=columns_name, index=columns_name)

        unknown_matrix = np.ones((n_subsets,n_subsets))
        np.fill_diagonal(unknown_matrix, 0)
        df_uncertainty = pd.DataFrame(unknown_matrix, columns=columns_name, index=columns_name)

        self.df_similarity = df_similarity
        self.df_dissimilarity = df_dissimilarity
        self.df_uncertainty = df_uncertainty

    def get_pairwise_subsets(self):
        """
        Return list of pairwise considered subsets.
        """
        pairwise_subsets = itertools.combinations(self.subsets, 2)
        return pairwise_subsets

    def __extract_evidence__(self, elements_i, elements_j):
        intersection_ai_aj = list((elements_i & elements_j).elements())
        combination_t = list((elements_i - elements_j).elements())
        combination_v = list((elements_j - elements_i).elements())
        n_i = len(list((elements_i).elements()))
        n_j = len(list((elements_j).elements()))

        # Extract evidence from Ai and Aj that their intersection is not empty
        if len(intersection_ai_aj) == 0 or len(combination_t) > self.rage_size_subset or len(combination_v) > self.rage_size_subset or (len(combination_t)==0 and len(combination_v)==0):
            return None
        else:
            union_ai_aj = list((elements_i | elements_j).elements())
            j_index = (np.sqrt(np.power(len(intersection_ai_aj),2)/(len(union_ai_aj)*np.min([n_i,n_j])))+1)/(2*np.max([len(combination_t), len(combination_v)]))
            return intersection_ai_aj, combination_t, combination_v, j_index
    
    def __collect_evidence__(self):
        return itertools.combinations(self.data, 2)
    
    def __model_evidence__(self):
        """
        Return list of similarity between considered subsets.
        """
        
        mass_functions = []
        pieces_of_evidence = self.__collect_evidence__()
        # for alloy_i, alloy_j in itertools.combinations(self.data, 2):
        for alloy_i, alloy_j in pieces_of_evidence:
            if np.abs(alloy_i[2]-alloy_j[2])<=2:
                evidence = self.__extract_evidence__(alloy_i[0], alloy_j[0])
                if evidence is not None:
                    alpha = alloy_i[3] * alloy_j[3] * self.alpha
                    intersection_ai_aj, combination_t, combination_v, j_index = evidence
                    # Evaluate the discounting factor for the source of evidence
                    if alloy_i[1] == alloy_j[1]:
                        mass_function = MassFunction(source=[({"similar"}, np.float64(alpha*j_index))], coreset={"similar", "dissimilar"})
                    else:
                        mass_function = MassFunction(source=[({"dissimilar"}, np.float64(alpha*j_index))], coreset={"similar", "dissimilar"})
                    mass_functions.append([intersection_ai_aj, combination_t, combination_v, mass_function])
        
        return mass_functions

    def __combine_evidence__(self, mass_functions):
        similar_set = frozenset({"similar"})
        dissimilar_set = frozenset({"dissimilar"})
        unknown_set = frozenset({"similar", "dissimilar"})
        
        for intersection_ai_aj, combination_t, combination_v, mass_function in mass_functions:
            index_t, index_v = "|".join(sorted(combination_t)), "|".join(sorted(combination_v))
            similar_score = self.df_similarity.loc[index_t, index_v]
            dissimilar_score = self.df_dissimilarity.loc[index_t, index_v]
            unk_score = self.df_uncertainty.loc[index_t, index_v]
            combined_mass_function = mass_function.combine(
                MassFunction(
                    source=[
                        ({"similar"}, np.float64(similar_score)), 
                        ({"dissimilar"}, np.float64(dissimilar_score)), 
                        ({"similar", "dissimilar"}, np.float64(unk_score))
                    ], coreset={"similar", "dissimilar"}
                )
            )
            
            # Update matrices
            self.df_similarity.loc[index_t, index_v] = combined_mass_function[similar_set]
            self.df_dissimilarity.loc[index_t, index_v] = combined_mass_function[dissimilar_set]
            self.df_uncertainty.loc[index_t, index_v] = combined_mass_function[unknown_set]
            self.df_similarity.loc[index_v, index_t] = combined_mass_function[similar_set]
            self.df_dissimilarity.loc[index_v, index_t] = combined_mass_function[dissimilar_set]
            self.df_uncertainty.loc[index_v, index_t] = combined_mass_function[unknown_set]

    def similarity_measurement(self, spark=False):
        """
        Return list of similarity between considered subsets.
        """
        mass_functions = self.__model_evidence__()
        self.__combine_evidence__(mass_functions)
        
    def to_csv(self, output_dir):
        """
        Save calculated data frames to files.

        output_dir: a path of the destination folder to save the data frames.
        """
        self.df_similarity.to_csv("{}/similarity.csv".format(output_dir))
        self.df_dissimilarity.to_csv("{}/dissimilarity.csv".format(output_dir))
        self.df_uncertainty.to_csv("{}/uncertainty.csv".format(output_dir))
    
    def to_parquet(self, output_dir):
        """
        Save calculated data frames to parquet files.

        output_dir: a path of the destination folder to save the data frames.
        """
        self.df_similarity.to_parquet("{}/similarity.parquet".format(output_dir))
        self.df_dissimilarity.to_parquet("{}/dissimilarity.parquet".format(output_dir))
        self.df_uncertainty.to_parquet("{}/uncertainty.parquet".format(output_dir))

class EvidentialClassifier(BaseEstimator):
    """
    An evidence-based classification module using Dempster-Shafer theory.
    The module uses mass function to model pieces of evidence about the property of the new instance, which are collected from the data set.
    It should be noted that the instance in the work is a multi-principal-element alloy (list of element).
    This module provides two running modes: single or parallel (using Spark).
    """

    def __init__(self, core_set, frame_of_discernment, seperate_symbol="|", n_gram_evidence=2, alpha=0.1, version='v5'):
        """
        Creates a new classifier.

        It is an object to classify new instance based on the observed data set.
        seperate_symbol: symbol is used to separate elements in the instances of the data set.
        n_gram_evidence: max length of the combinations of element which are used to generate the new instance from observed instances based on substitution method.
        alpha (or alpha): a hyperparameter is used to handle the conflict between pieces of evidence.
        """
        self.seperate_symbol = seperate_symbol
        self.n_gram_evidence = n_gram_evidence
        self.alpha = alpha
        self.version = version
        self.core_set = core_set
        self.frame_of_discernment = frozenset(frame_of_discernment)
        
    def fit(self, X, y, spark=False, weights=None):
        # Parse the data into the form of Counter objects
        self.data = []
        if weights is None:
            weights = np.ones(len(X))
        for set_name, label, w in zip(X, y, weights):
            set_name = set_name.split(self.seperate_symbol)
            self.data.append(tuple([Counter(set_name), label, len(set_name), w]))
            
        # # Construct the nn graph of evidence collector
        # self.__learn_evidence__collector__()
        
        # Measure similarity between materials
        similarity_information = SimilarityCombinationElement(
            data=self.data, rage_size_subset=self.n_gram_evidence, seperate_symbol=self.seperate_symbol, 
            alpha=self.alpha, core_set=self.core_set
        )
        similarity_information.similarity_measurement(spark)
        self.similarity_information = similarity_information

    def score(self, X, y):
        y_pred = self.predict(X)
        return accuracy_score(y, y_pred)
        
    def __extract_evidence__(self, alloy_new, alloy_k):
        c_intersection = list((alloy_k & alloy_new).elements())
        n_new = len(list((alloy_new).elements()))
        n_k = len(list((alloy_k).elements()))
        if len(c_intersection)!= 0 and alloy_k != alloy_new:
            combination_t = list((alloy_k - alloy_new).elements())
            combination_v = list((alloy_new - alloy_k).elements())
            if len(combination_t) <= self.n_gram_evidence and len(combination_v) <= self.n_gram_evidence: 
                index_t, index_v = "|".join(sorted(combination_t)), "|".join(sorted(combination_v))
                similar_score = self.similarity_information.df_similarity.loc[index_t, index_v]
                dissimilar_score = self.similarity_information.df_dissimilarity.loc[index_t, index_v]
                unknown_score = self.similarity_information.df_uncertainty.loc[index_t, index_v]
                if unknown_score != 1:
                    c_union = list((alloy_k | alloy_new).elements())
                    j_index =(np.sqrt(np.power(len(c_intersection),2)/(len(c_union)*np.min([n_new,n_k])))+1)/(2*np.max([len(combination_t), len(combination_v)]))
                    return combination_t, combination_v, similar_score, dissimilar_score, unknown_score, j_index
                else:
                    return None
            else:
                return None
        return None
    
    def __modelling_belief__(self, material):
        # Init the variables
        pieces_of_evidence = []
        material = material.split(self.seperate_symbol)
        candidate = Counter(material)
        final_decision = MassFunction(coreset=self.frame_of_discernment)
        for data in self.data:
            alloy_k, label_k, len_k, w_k = data
            if np.abs(len(material)-len_k) <= 2:
                evidence = self.__extract_evidence__(candidate, alloy_k)
                if evidence is not None:
                    combination_t, combination_v, similar_score, dissimilar_score, unknown_score, j_index = evidence
                    # Model the evidence
                    discounting_factor = j_index * w_k * self.alpha
                    # discounting_factor = (1 if label_k == "High" else 0.01)*j_index
                    mass_function = MassFunction(
                        source=[
                            ({label_k}, np.float64(discounting_factor*similar_score)),
                            (self.frame_of_discernment.difference(frozenset({label_k})), np.float64(discounting_factor*dissimilar_score))
                        ], 
                        coreset=self.frame_of_discernment
                    )
                    # Combine the evidence
                    final_decision = final_decision.combine(mass_function)
                    # Save the modeled evidence
                    pieces_of_evidence.append([alloy_k, combination_t, combination_v, mass_function])
        return pieces_of_evidence, final_decision
    
    def save_similarity_matrices(self, output_dir):
        """
        Save similarity information to data frames.

        output_dir: a path of the destination folder to save the data frames.
        """
        self.similarity_information.to_parquet(output_dir)
        
    def predict(self, X, trace=False, spark=False, show_decision=False):
        """
        Returns the predicted labels of new instances (materials).

        trace: If True, the function adds lists of pieces of evidence, which are used to estimate properties of the new instances, to the output.
        show_decision: If True, the function adds the combined mass function of collected pieces of evidence to the output.
        spark: If True, the function is deployed on the Spark system to accelerate runtime.
        """
        y_pred = []
        trace_pred = []
        final_decisions = []
        
        for material in X:
            pieces_of_evidence, final_decision = self.__modelling_belief__(material)
            pignistic = final_decision.pignistic()
            pred_label = np.array(list(final_decision.coreset))[np.argmax(pignistic)]
            y_pred.append(pred_label)
            trace_pred.append(pieces_of_evidence)
            final_decisions.append(final_decision)
        if show_decision and trace:
            return y_pred, final_decisions, trace_pred
        elif trace:
            return y_pred, trace_pred
        elif show_decision:
            return y_pred, final_decisions
        else:
            return y_pred